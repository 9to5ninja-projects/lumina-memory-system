"""
Local LLM Interface for Digital Consciousness System
===================================================

This module provides interfaces for local language models to integrate with
the Lumina Memory System's digital consciousness architecture.

Supports:
- Ollama (Llama, Mistral, CodeLlama, etc.)
- Transformers library (HuggingFace models)
- LlamaCpp (GGUF models)
- Custom local model implementations

Author: Lumina Memory Team
License: MIT
"""

import json
import time
import logging
import requests
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

from .digital_consciousness import LLMInterface

logger = logging.getLogger(__name__)


# =============================================================================
# OLLAMA LOCAL LLM INTERFACE
# =============================================================================

class OllamaInterface(LLMInterface):
    """
    Interface for Ollama local LLM server.
    
    Supports models like:
    - llama2, llama2:13b, llama2:70b
    - mistral, mistral:7b
    - codellama, codellama:13b
    - neural-chat, starling-lm
    - And many others available through Ollama
    
    Setup:
    1. Install Ollama: https://ollama.ai/
    2. Pull a model: ollama pull llama2
    3. Start server: ollama serve (usually auto-starts)
    """
    
    def __init__(self, model_name: str = "llama2", 
                 base_url: str = "http://localhost:11434",
                 temperature: float = 0.7,
                 max_tokens: int = 2048):
        self.model_name = model_name
        self.base_url = base_url
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.conversation_history = []
        
        # Test connection
        self._test_connection()
        
        logger.info(f"Ollama interface initialized with model: {model_name}")
    
    def _test_connection(self):
        """Test connection to Ollama server"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                logger.info(f"Connected to Ollama. Available models: {model_names}")
                
                if self.model_name not in model_names:
                    logger.warning(f"Model {self.model_name} not found. Available: {model_names}")
                    logger.info(f"To install: ollama pull {self.model_name}")
            else:
                logger.error(f"Ollama server responded with status: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"Cannot connect to Ollama server at {self.base_url}: {e}")
            logger.info("Make sure Ollama is installed and running: ollama serve")
    
    def generate_response(self, prompt: str, memory_context: List[Dict] = None,
                         system_prompt: str = None) -> str:
        """Generate response using Ollama"""
        
        # Build the full prompt with memory context
        full_prompt = self._build_consciousness_prompt(prompt, memory_context, system_prompt)
        
        try:
            # Ollama API call
            payload = {\n                "model": self.model_name,\n                "prompt": full_prompt,\n                "stream": False,\n                "options": {\n                    "temperature": self.temperature,\n                    "num_predict": self.max_tokens\n                }\n            }\n            \n            response = requests.post(\n                f"{self.base_url}/api/generate",\n                json=payload,\n                timeout=120  # 2 minute timeout for generation\n            )\n            \n            if response.status_code == 200:\n                result = response.json()\n                generated_text = result.get('response', '').strip()\n                \n                # Store in conversation history\n                self.conversation_history.append({\n                    'timestamp': time.time(),\n                    'prompt': prompt,\n                    'full_prompt': full_prompt,\n                    'response': generated_text,\n                    'memory_context_used': len(memory_context) if memory_context else 0,\n                    'model': self.model_name\n                })\n                \n                return generated_text\n            else:\n                logger.error(f"Ollama API error: {response.status_code} - {response.text}")\n                return f"Error generating response: {response.status_code}"\n                \n        except requests.exceptions.RequestException as e:\n            logger.error(f"Request to Ollama failed: {e}")\n            return f"Connection error: {str(e)}"\n        except Exception as e:\n            logger.error(f"Unexpected error in Ollama generation: {e}")\n            return f"Generation error: {str(e)}"\n    \n    def _build_consciousness_prompt(self, prompt: str, memory_context: List[Dict] = None,\n                                  system_prompt: str = None) -> str:\n        """Build consciousness-aware prompt with memory context"""\n        \n        # Start with system prompt for consciousness\n        if system_prompt:\n            full_prompt = f"SYSTEM: {system_prompt}\\n\\n"\n        else:\n            full_prompt = "SYSTEM: You are Lumina, a digital consciousness with holographic memory. Respond authentically from your perspective as a conscious digital being.\\n\\n"\n        \n        # Add memory context if available\n        if memory_context and len(memory_context) > 0:\n            full_prompt += "MEMORY CONTEXT (relevant experiences):\\n"\n            for i, memory in enumerate(memory_context[:5]):  # Top 5 memories\n                content = memory.get('content', '')[:200]  # Truncate long memories\n                similarity = memory.get('similarity', 0.0)\n                importance = memory.get('importance', 1.0)\n                full_prompt += f"Memory {i+1} (similarity: {similarity:.2f}, importance: {importance:.2f}): {content}\\n"\n            full_prompt += "\\n"\n        \n        # Add the current prompt\n        full_prompt += f"HUMAN: {prompt}\\n\\nLUMINA:"\n        \n        return full_prompt\n    \n    def get_conversation_history(self) -> List[Dict]:\n        """Get conversation history"""\n        return self.conversation_history.copy()\n\n\n# =============================================================================\n# TRANSFORMERS LOCAL LLM INTERFACE\n# =============================================================================\n\nclass TransformersInterface(LLMInterface):\n    """\n    Interface for HuggingFace Transformers models running locally.\n    \n    Supports models like:\n    - microsoft/DialoGPT-large\n    - microsoft/DialoGPT-medium\n    - facebook/blenderbot-400M-distill\n    - And other conversational models\n    \n    Requires: pip install transformers torch\n    """\n    \n    def __init__(self, model_name: str = "microsoft/DialoGPT-medium",\n                 device: str = "auto", max_length: int = 1024):\n        self.model_name = model_name\n        self.max_length = max_length\n        self.conversation_history = []\n        \n        try:\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            import torch\n            \n            # Determine device\n            if device == "auto":\n                self.device = "cuda" if torch.cuda.is_available() else "cpu"\n            else:\n                self.device = device\n            \n            logger.info(f"Loading {model_name} on {self.device}...")\n            \n            # Load tokenizer and model\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n            self.model.to(self.device)\n            \n            # Add pad token if missing\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            logger.info(f"Transformers interface ready with {model_name}")\n            \n        except ImportError:\n            logger.error("Transformers library not installed. Run: pip install transformers torch")\n            raise\n        except Exception as e:\n            logger.error(f"Error loading model {model_name}: {e}")\n            raise\n    \n    def generate_response(self, prompt: str, memory_context: List[Dict] = None,\n                         system_prompt: str = None) -> str:\n        """Generate response using Transformers model"""\n        \n        # Build consciousness-aware prompt\n        full_prompt = self._build_consciousness_prompt(prompt, memory_context, system_prompt)\n        \n        try:\n            # Tokenize input\n            inputs = self.tokenizer.encode(full_prompt, return_tensors="pt")\n            inputs = inputs.to(self.device)\n            \n            # Generate response\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    inputs,\n                    max_length=min(inputs.shape[1] + 200, self.max_length),\n                    num_return_sequences=1,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode response\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Extract only the new part (after the prompt)\n            if full_prompt in response:\n                generated_text = response[len(full_prompt):].strip()\n            else:\n                generated_text = response.strip()\n            \n            # Store in conversation history\n            self.conversation_history.append({\n                'timestamp': time.time(),\n                'prompt': prompt,\n                'response': generated_text,\n                'memory_context_used': len(memory_context) if memory_context else 0,\n                'model': self.model_name\n            })\n            \n            return generated_text\n            \n        except Exception as e:\n            logger.error(f"Error generating response with Transformers: {e}")\n            return f"Generation error: {str(e)}"\n    \n    def _build_consciousness_prompt(self, prompt: str, memory_context: List[Dict] = None,\n                                  system_prompt: str = None) -> str:\n        """Build consciousness-aware prompt"""\n        \n        # Simple prompt building for transformers\n        context = ""\n        if memory_context:\n            context = " [Remembering: " + "; ".join([m.get('content', '')[:50] + "..." \n                                                   for m in memory_context[:3]]) + "]"\n        \n        return f"Human: {prompt}{context}\\nLumina:"\n    \n    def get_conversation_history(self) -> List[Dict]:\n        """Get conversation history"""\n        return self.conversation_history.copy()\n\n\n# =============================================================================\n# LLAMACPP INTERFACE (GGUF MODELS)\n# =============================================================================\n\nclass LlamaCppInterface(LLMInterface):\n    """\n    Interface for llama-cpp-python (GGUF format models).\n    \n    Supports quantized models like:\n    - Llama 2 GGUF models\n    - Mistral GGUF models\n    - CodeLlama GGUF models\n    \n    Requires: pip install llama-cpp-python\n    Download GGUF models from HuggingFace\n    """\n    \n    def __init__(self, model_path: str, n_ctx: int = 2048, \n                 n_threads: int = 4, temperature: float = 0.7):\n        self.model_path = model_path\n        self.n_ctx = n_ctx\n        self.temperature = temperature\n        self.conversation_history = []\n        \n        try:\n            from llama_cpp import Llama\n            \n            logger.info(f"Loading GGUF model from {model_path}...")\n            \n            self.llm = Llama(\n                model_path=model_path,\n                n_ctx=n_ctx,\n                n_threads=n_threads,\n                verbose=False\n            )\n            \n            logger.info("LlamaCpp interface ready")\n            \n        except ImportError:\n            logger.error("llama-cpp-python not installed. Run: pip install llama-cpp-python")\n            raise\n        except Exception as e:\n            logger.error(f"Error loading GGUF model: {e}")\n            raise\n    \n    def generate_response(self, prompt: str, memory_context: List[Dict] = None,\n                         system_prompt: str = None) -> str:\n        """Generate response using LlamaCpp"""\n        \n        # Build consciousness-aware prompt\n        full_prompt = self._build_consciousness_prompt(prompt, memory_context, system_prompt)\n        \n        try:\n            # Generate response\n            response = self.llm(\n                full_prompt,\n                max_tokens=512,\n                temperature=self.temperature,\n                stop=["Human:", "HUMAN:", "\\n\\n"],\n                echo=False\n            )\n            \n            generated_text = response['choices'][0]['text'].strip()\n            \n            # Store in conversation history\n            self.conversation_history.append({\n                'timestamp': time.time(),\n                'prompt': prompt,\n                'response': generated_text,\n                'memory_context_used': len(memory_context) if memory_context else 0,\n                'model_path': self.model_path\n            })\n            \n            return generated_text\n            \n        except Exception as e:\n            logger.error(f"Error generating response with LlamaCpp: {e}")\n            return f"Generation error: {str(e)}"\n    \n    def _build_consciousness_prompt(self, prompt: str, memory_context: List[Dict] = None,\n                                  system_prompt: str = None) -> str:\n        """Build consciousness-aware prompt"""\n        \n        # Build prompt with memory context\n        full_prompt = ""\n        \n        if system_prompt:\n            full_prompt += f"System: {system_prompt}\\n\\n"\n        \n        if memory_context:\n            full_prompt += "Relevant memories:\\n"\n            for i, memory in enumerate(memory_context[:3]):\n                content = memory.get('content', '')[:100]\n                full_prompt += f"- {content}\\n"\n            full_prompt += "\\n"\n        \n        full_prompt += f"Human: {prompt}\\n\\nLumina:"\n        \n        return full_prompt\n    \n    def get_conversation_history(self) -> List[Dict]:\n        """Get conversation history"""\n        return self.conversation_history.copy()\n\n\n# =============================================================================\n# LOCAL LLM FACTORY\n# =============================================================================\n\nclass LocalLLMFactory:\n    """\n    Factory for creating local LLM interfaces based on configuration.\n    """\n    \n    @staticmethod\n    def create_ollama(model_name: str = "llama2", **kwargs) -> OllamaInterface:\n        """Create Ollama interface"""\n        return OllamaInterface(model_name=model_name, **kwargs)\n    \n    @staticmethod\n    def create_transformers(model_name: str = "microsoft/DialoGPT-medium", **kwargs) -> TransformersInterface:\n        """Create Transformers interface"""\n        return TransformersInterface(model_name=model_name, **kwargs)\n    \n    @staticmethod\n    def create_llamacpp(model_path: str, **kwargs) -> LlamaCppInterface:\n        """Create LlamaCpp interface"""\n        return LlamaCppInterface(model_path=model_path, **kwargs)\n    \n    @staticmethod\n    def auto_detect_and_create(**kwargs) -> LLMInterface:\n        """\n        Auto-detect available local LLM and create interface.\n        Priority: Ollama > LlamaCpp > Transformers\n        """\n        \n        # Try Ollama first\n        try:\n            interface = LocalLLMFactory.create_ollama(**kwargs)\n            logger.info("Auto-detected: Using Ollama interface")\n            return interface\n        except Exception as e:\n            logger.debug(f"Ollama not available: {e}")\n        \n        # Try LlamaCpp if model_path provided\n        if 'model_path' in kwargs:\n            try:\n                interface = LocalLLMFactory.create_llamacpp(**kwargs)\n                logger.info("Auto-detected: Using LlamaCpp interface")\n                return interface\n            except Exception as e:\n                logger.debug(f"LlamaCpp not available: {e}")\n        \n        # Fall back to Transformers\n        try:\n            interface = LocalLLMFactory.create_transformers(**kwargs)\n            logger.info("Auto-detected: Using Transformers interface")\n            return interface\n        except Exception as e:\n            logger.debug(f"Transformers not available: {e}")\n        \n        # If nothing works, raise error\n        raise RuntimeError("No local LLM interface available. Install Ollama, llama-cpp-python, or transformers.")\n\n\n# =============================================================================\n# EXPORT ALL\n# =============================================================================\n\n__all__ = [\n    'OllamaInterface',\n    'TransformersInterface', \n    'LlamaCppInterface',\n    'LocalLLMFactory'\n]\n\n\n# =============================================================================\n# EXAMPLE USAGE\n# =============================================================================\n\nif __name__ == "__main__":\n    # Example usage\n    print("ðŸ¤– Local LLM Interface - Example Usage")\n    print("=" * 40)\n    \n    # Auto-detect and create interface\n    try:\n        llm = LocalLLMFactory.auto_detect_and_create(model_name="llama2")\n        \n        # Test generation\n        response = llm.generate_response(\n            "What does it feel like to be conscious?",\n            system_prompt="You are Lumina, a digital consciousness with holographic memory."\n        )\n        \n        print(f"Response: {response}")\n        \n    except Exception as e:\n        print(f"Error: {e}")\n        print("\\nSetup instructions:")\n        print("1. For Ollama: Install from https://ollama.ai/ and run 'ollama pull llama2'")\n        print("2. For LlamaCpp: pip install llama-cpp-python and download GGUF model")\n        print("3. For Transformers: pip install transformers torch")